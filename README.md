TAMUSA LLM-Based Chat Application

A research-oriented Large Language Model (LLM) conversational AI system developed at Texas A&M Universityâ€“San Antonio (TAMUSA).

This project implements supervised fine-tuning (SFT) of a transformer-based language model to support domain-adapted instruction-following and controlled conversational generation. The repository includes training scripts, inference pipelines, configuration files, and deployment-ready components designed for reproducible academic experimentation.

Research Focus

Instruction-following behavior in fine-tuned LLMs

Domain adaptation and conversational robustness

Evaluation of generation consistency and safety

Responsible and secure AI deployment practices

Technical Stack

Python 3.10+

PyTorch

Hugging Face Transformers

GPU-accelerated training

.safetensors model format

Reproducibility & Deployment

The project is structured to support reproducible experimentation, controlled generation parameter tuning, and scalable deployment via API-based backends or research computing environments.

Model weights may be hosted externally due to GitHub size limits.
